"""
Document Parsers for Academic Writing Skills
Support for LaTeX and Typst document parsing.
"""

import re
from abc import ABC, abstractmethod
from typing import Any


class DocumentParser(ABC):
    """Abstract base class for document parsers."""

    @abstractmethod
    def split_sections(self, content: str) -> dict[str, tuple[int, int]]:
        """
        Split document into sections.
        Returns map of {section_name: (start_line, end_line)}.
        """
        pass

    @abstractmethod
    def extract_visible_text(self, line: str) -> str:
        """
        Extract text visible to reader, preserving structure markers.
        Used for line-by-line AI trace checking.
        """
        pass

    @abstractmethod
    def clean_text(self, content: str, keep_structure: bool = False) -> str:
        """
        Extract pure prose text, removing all markup.
        Used for prose extraction and word counting.
        """
        pass

    @abstractmethod
    def get_comment_prefix(self) -> str:
        """Get the comment prefix for the language."""
        pass


class LatexParser(DocumentParser):
    """Parser for LaTeX documents."""

    # Section patterns
    SECTION_PATTERNS = {
        "abstract": r"\\begin{abstract}|\\section*?{abstract}?",
        "introduction": r"\\section*?{Introduction}|\\section*?{INTRODUCTION}",
        "related": r"\\section*?{Related\s+Work}|\\section*?{RELATED\s+WORK}",
        "method": r"\\section*?{.*(?:Method|Methodology|Approach)}",
        "experiment": r"\\section*?{.*(?:Experiment|Evaluation|Implementation)}",
        "result": r"\\section*?{.*(?:Result|Performance)}",
        "discussion": r"\\section*?{.*(?:Discussion|Analysis)}",
        "conclusion": r"\\section*?{.*(?:Conclusion|Conclusions)}",
    }

    # Preservation patterns for visible text extraction
    PRESERVE_PATTERNS = [
        r"\\cite{[^}]+}",  # Citations
        r"\\ref{[^}]+}",  # References
        r"\\label{[^}]+}",  # Labels
        r"\\eqref{[^}]+}",  # Equation references
        r"\\autoref{[^}]+}",  # Auto references
        r"\$\$[^$]*\$\$",  # Display math
        r"\$[^$]*\$",  # Inline math
        r"\\begin{equation}.*?\\end{equation}",  # Equations
        r"\\begin{align}.*?\\end{align}",  # Align environments
        r"\\begin{.*?}.*?\\end{.*?}",  # Generic environments
        r"\\includegraphics(?:\[[^]]*\])?\{[^}]+\}",  # Images
        r"\\caption{[^}]+}",  # Captions
    ]

    # Environments to skip entirely for clean text
    SKIP_ENVIRONMENTS = [
        "equation",
        "equation*",
        "align",
        "align*",
        "gather",
        "gather*",
        "multline",
        "multline*",
        "eqnarray",
        "eqnarray*",
        "displaymath",
        "figure",
        "figure*",
        "table",
        "table*",
        "tabular",
        "tabular*",
        "lstlisting",
        "verbatim",
        "minted",
        "algorithm",
        "algorithmic",
    ]

    def get_comment_prefix(self) -> str:
        return "%"

    def split_sections(self, content: str) -> dict[str, tuple[int, int]]:
        lines = content.split("\n")
        sections = {}
        current_section = "preamble"
        start_line = 0

        for i, line in enumerate(lines, 1):
            for section_name, pattern in self.SECTION_PATTERNS.items():
                if re.search(pattern, line, re.IGNORECASE):
                    if current_section != "preamble":
                        sections[current_section] = (start_line, i - 1)
                    current_section = section_name
                    start_line = i
                    break

        # Last section
        if current_section != "preamble":
            sections[current_section] = (start_line, len(lines))

        return sections

    def extract_visible_text(self, line: str) -> str:
        # Preserve structure markers logic
        preserved = []
        temp_line = line

        for pattern in self.PRESERVE_PATTERNS:
            matches = list(re.finditer(pattern, temp_line, re.DOTALL))
            for match in reversed(matches):
                preserved.append(
                    {"start": match.start(), "end": match.end(), "text": match.group()}
                )
                # Replace with placeholder of same length to keep indices valid
                placeholder = " " * (match.end() - match.start())
                temp_line = temp_line[: match.start()] + placeholder + temp_line[match.end() :]

        preserved.sort(key=lambda x: x["start"])

        visible_parts = []
        last_end = 0

        for item in preserved:
            if item["start"] > last_end:
                visible_parts.append(temp_line[last_end : item["start"]])
            last_end = item["end"]

        if last_end < len(temp_line):
            visible_parts.append(temp_line[last_end:])

        return " ".join(visible_parts).strip()

    def clean_text(self, content: str, keep_structure: bool = False) -> str:
        # Remove comments
        content = re.sub(r"(?<!\\)%.*", "", content)

        # Remove skip environments
        for env in self.SKIP_ENVIRONMENTS:
            pattern = rf"\\begin{{{env}}}.*?\\end{{{env}}}"
            content = re.sub(pattern, "", content, flags=re.DOTALL)

        # Remove inline math
        content = re.sub(r"\$[^$]+\$", "", content)
        content = re.sub(r"\\[^]]*\\]", "", content, flags=re.DOTALL)
        content = re.sub(r"\\\(.*?\\\)", "", content, flags=re.DOTALL)

        # Formatting
        if keep_structure:
            content = re.sub(r"\\section\*?{([^}]+)}", r"\n\n## \1\n\n", content)
            content = re.sub(r"\\subsection\*?{([^}]+)}", r"\n\n### \1\n\n", content)
        else:
            content = re.sub(r"\\(?:sub)*section\*?{[^}]+}", "", content)

        # Remove commands
        content = re.sub(r"\\[a-zA-Z]+\*?(?:\[[^\]]*\])?{([^}]*)}", r"\1", content)
        content = re.sub(r"\\[a-zA-Z]+\*?", "", content)
        content = re.sub(r"[{}]", "", content)

        # Cleanup whitespace
        content = re.sub(r"\n+", "\n", content)
        content = re.sub(r" +", " ", content)
        content = re.sub(r"\.(\s*\.)+", ".", content)  # Fix multiple periods

        return content.strip()


class TypstParser(DocumentParser):
    """Parser for Typst documents."""

    # Section patterns (Heading 1-3)
    # Matches: = Introduction or == Related Work
    SECTION_PATTERNS = {
        "introduction": r"^=\s+(?:Introduction|INTRODUCTION)",
        "related": r"^=\s+(?:Related\s+Work|RELATED\s+WORK)",
        "method": r"^=\s+.*(?:Method|Methodology|Approach)",
        "experiment": r"^=\s+.*(?:Experiment|Evaluation|Implementation)",
        "result": r"^=\s+.*(?:Result|Performance)",
        "discussion": r"^=\s+.*(?:Discussion|Analysis)",
        "conclusion": r"^=\s+.*(?:Conclusion|Conclusions)",
        "abstract": r"#abstract\[",
    }

    PRESERVE_PATTERNS = [
        r"@[a-zA-Z0-9_-]+",  # Citations @key
        r"#cite\([^)]+\)",  # Function calls #cite()
        r"#figure\([^)]+\)",  # Figures
        r"#table\([^)]+\)",  # Tables
        r"\$[^$]+\$",  # Math $...$
        r"//.*",  # Line comments
        r"/\*.*?\*/",  # Block comments
        r"<[a-zA-Z0-9_-]+>",  # Labels <label>
        r"#link\([^)]+\)",  # Links
    ]

    def get_comment_prefix(self) -> str:
        return "//"

    def split_sections(self, content: str) -> dict[str, tuple[int, int]]:
        lines = content.split("\n")
        sections = {}
        current_section = "preamble"
        start_line = 0

        for i, line in enumerate(lines, 1):
            line = line.strip()
            # Ignore comments
            if line.startswith("//"):
                continue

            for section_name, pattern in self.SECTION_PATTERNS.items():
                if re.search(pattern, line, re.IGNORECASE):
                    if current_section != "preamble":
                        sections[current_section] = (start_line, i - 1)
                    current_section = section_name
                    start_line = i
                    break

        if current_section != "preamble":
            sections[current_section] = (start_line, len(lines))

        return sections

    def extract_visible_text(self, line: str) -> str:
        # Same logic as LatexParser but with Typst patterns
        temp_line = line

        # Remove comments first for Typst
        if "//" in temp_line:
            temp_line = temp_line.split("//")[0]

        preserved = []
        for pattern in self.PRESERVE_PATTERNS:
            matches = list(re.finditer(pattern, temp_line, re.DOTALL))
            for match in reversed(matches):
                preserved.append(
                    {"start": match.start(), "end": match.end(), "text": match.group()}
                )
                placeholder = " " * (match.end() - match.start())
                temp_line = temp_line[: match.start()] + placeholder + temp_line[match.end() :]

        preserved.sort(key=lambda x: x["start"])

        # Extract visible logic... (duplicated, could refactor to base)
        # But keeping separate for now to be safe
        visible_parts = []
        last_end = 0
        for item in preserved:
            if item["start"] > last_end:
                visible_parts.append(temp_line[last_end : item["start"]])
            last_end = item["end"]
        if last_end < len(temp_line):
            visible_parts.append(temp_line[last_end:])

        return " ".join(visible_parts).strip()

    def clean_text(self, content: str, keep_structure: bool = False) -> str:
        # Remove comments
        content = re.sub(r"//.*", "", content)
        content = re.sub(r"/\*.*?\*/", "", content, flags=re.DOTALL)

        # Remove math
        content = re.sub(r"\$[^$]+\$", "", content)

        # Handle headers
        if keep_structure:
            content = re.sub(r"^=+\s+(.+)$", r"\n\n## \1\n\n", content, flags=re.MULTILINE)
        else:
            content = re.sub(r"^=+\s+.+$", "", content, flags=re.MULTILINE)

        # Remove function calls #func(...) - basic support
        # This is hard with regex due to nested parenthesis, simplified here
        content = re.sub(r"#[a-zA-Z0-9_]+\([^)]*\)", "", content)
        content = re.sub(r"@[a-zA-Z0-9_-]+", "", content)
        content = re.sub(r"<[a-zA-Z0-9_-]+>", "", content)

        # Cleanup
        content = re.sub(r"\n+", "\n", content)
        content = re.sub(r" +", " ", content)
        content = re.sub(r"\.(\s*\.)+", ".", content)  # Fix multiple periods
        return content.strip()


def get_parser(file_path: Any) -> DocumentParser:
    """Factory method to get appropriate parser."""
    path_str = str(file_path).lower()
    if path_str.endswith(".typ"):
        return TypstParser()
    return LatexParser()


def _normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def _extract_balanced_block(content: str, start_idx: int, opener: str, closer: str) -> str:
    """Extract a balanced block body from content starting at opener index."""
    if start_idx < 0 or start_idx >= len(content) or content[start_idx] != opener:
        return ""

    depth = 0
    block_start = -1

    for i in range(start_idx, len(content)):
        char = content[i]
        if char == opener:
            depth += 1
            if depth == 1:
                block_start = i + 1
        elif char == closer:
            depth -= 1
            if depth == 0 and block_start >= 0:
                return content[block_start:i]
            if depth < 0:
                return ""
    return ""


def _strip_typst_markup(text: str) -> str:
    """Strip lightweight Typst markup for title/abstract extraction."""
    cleaned = text
    cleaned = re.sub(r"#\w+\([^)]*\)", " ", cleaned)

    # Collapse simple bracket macros repeatedly (e.g., #emph[Paper], nested forms).
    while True:
        collapsed = re.sub(r"#\w+\[([^\[\]]*)\]", r"\1", cleaned)
        if collapsed == cleaned:
            break
        cleaned = collapsed

    cleaned = re.sub(r"[\[\]]", " ", cleaned)
    return _normalize_whitespace(cleaned)


def _strip_latex_markup(text: str) -> str:
    """Strip lightweight LaTeX markup for title/abstract extraction."""
    cleaned = text
    cleaned = re.sub(r"(?<!\\)%.*", "", cleaned)
    cleaned = re.sub(r"\$[^$]*\$", " ", cleaned)
    cleaned = re.sub(r"\\[a-zA-Z]+\*?(?:\[[^\]]*\])?{([^{}]*)}", r"\1", cleaned)
    cleaned = re.sub(r"\\[a-zA-Z]+\*?", " ", cleaned)
    cleaned = re.sub(r"[{}]", " ", cleaned)
    return _normalize_whitespace(cleaned)


def extract_title(content: str) -> str:
    """Extract document title from LaTeX/Typst source content."""
    # LaTeX: \title{...}
    latex_match = re.search(r"\\title(?:\[[^\]]*\])?\{(.+?)\}", content, re.DOTALL)
    if latex_match:
        return _strip_latex_markup(latex_match.group(1))

    # Typst common: #set document(title: "...")
    typst_str = re.search(
        r"#set\s+document\s*\(\s*title\s*:\s*\"([^\"]+)\"",
        content,
        re.DOTALL,
    )
    if typst_str:
        return _normalize_whitespace(typst_str.group(1))

    # Typst bracket style: #set document(title: [ ... ])
    typst_block = re.search(r"#set\s+document\s*\(\s*title\s*:\s*\[", content, re.DOTALL)
    if typst_block:
        bracket_idx = typst_block.end() - 1
        text = _extract_balanced_block(content, bracket_idx, "[", "]")
        if text:
            return _strip_typst_markup(text)

    return ""


def extract_abstract(content: str) -> str:
    """Extract abstract text from LaTeX/Typst source content."""
    # LaTeX abstract environment
    latex_abs = re.search(r"\\begin{abstract}(.*?)\\end{abstract}", content, re.DOTALL)
    if latex_abs:
        return _strip_latex_markup(latex_abs.group(1))

    # LaTeX section-based abstract
    sec_abs = re.search(
        r"\\section\*?\{Abstract\}(.*?)(?=\\section\*?\{|\\end\{document\}|\Z)",
        content,
        re.DOTALL | re.IGNORECASE,
    )
    if sec_abs:
        return _strip_latex_markup(sec_abs.group(1))

    # Typst: #abstract[...]
    typst_abs = re.search(r"#abstract\[", content, re.DOTALL)
    if typst_abs:
        bracket_idx = typst_abs.end() - 1
        text = _extract_balanced_block(content, bracket_idx, "[", "]")
        if text:
            return _strip_typst_markup(text)

    return ""


def extract_latex_citation_keys(content: str) -> set[str]:
    """
    Extract citation keys from LaTeX source.
    Supports \\cite, \\citep, \\citet, \\nocite and variants with optional arguments.
    """
    keys: set[str] = set()
    pattern = re.compile(r"\\(?:cite\w*|nocite)\*?(?:\[[^\]]*\]\s*)*\{([^}]*)\}")
    for match in pattern.finditer(content):
        raw = match.group(1)
        for key in raw.split(","):
            normalized = key.strip()
            if normalized:
                keys.add(normalized)
    return keys
